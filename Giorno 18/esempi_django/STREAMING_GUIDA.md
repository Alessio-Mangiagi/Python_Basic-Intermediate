# ğŸ“¡ STREAMING CHAT - GUIDA TECNICA

## ğŸ¯ Cos'Ã¨ lo Streaming

Lo **streaming** permette di ricevere la risposta del chatbot **pezzo per pezzo** in tempo reale, invece di aspettare la risposta completa. Ãˆ simile a come ChatGPT mostra le risposte mentre vengono generate.

---

## ğŸ”§ Come Funziona il Codice

### 1. **Frontend - Interfaccia Utente**

Nel file `chatbot_web/templates/chatbot_web/chat.html`:

```html
<!-- Checkbox per attivare lo streaming -->
<label class="streaming-toggle">
    <input type="checkbox" id="streamingCheckbox" />
    <span class="streaming-label">ğŸ“¡ Streaming</span>
</label>
```

**JavaScript aggiornato:**

```javascript
// Controlla se lo streaming Ã¨ attivato
const isStreaming = streamingCheckbox.checked;

// Invia il parametro streaming nella richiesta
body: JSON.stringify({
    message: message,
    session_id: sessionId,
    streaming: isStreaming,  // â† Nuovo parametro
}),
```

### 2. **Serializer - Validazione Dati**

Nel file `chatbot_api/serializers.py`:

```python
class ChatRequestSerializer(serializers.Serializer):
    message = serializers.CharField(max_length=1000)
    session_id = serializers.CharField(max_length=100, required=False)

    # â† NUOVO: Campo per attivare streaming
    streaming = serializers.BooleanField(required=False, default=False)

class ChatResponseSerializer(serializers.Serializer):
    response = serializers.CharField()
    session_id = serializers.CharField()
    success = serializers.BooleanField()

    # â† NUOVI: Campi per gestire streaming
    is_streaming = serializers.BooleanField(default=False)
    is_final = serializers.BooleanField(default=True)
    error = serializers.CharField(required=False)
```

### 3. **Views - Endpoint API**

Nel file `chatbot_api/views.py`:

```python
def chat_endpoint(request):
    # Estrae il parametro streaming dalla richiesta
    streaming = serializer.validated_data.get("streaming", False)

    # Passa il parametro al servizio chatbot
    result = chatbot_service.chat(user_message, session_id, streaming)
```

### 4. **Service - Logica Business**

Nel file `chatbot_api/services.py`:

```python
def chat(self, user_message: str, session_id: str = None, streaming: bool = False):
    """
    Metodo principale che decide tra streaming e normale
    """
    # Prepara i messaggi per OpenAI
    messages.append({"role": "user", "content": user_message})

    # Decide quale metodo usare
    if streaming:
        return self._chat_with_streaming(messages, user_message, session_id)
    else:
        return self._chat_normal(messages, user_message, session_id)

def _chat_normal(self, messages, user_message, session_id):
    """
    Chat normale - risposta completa in una volta
    """
    response = openai.chat.completions.create(
        model="gpt-4.1-nano",
        messages=messages,
        max_tokens=150,
        stream=False,  # â† NO streaming
    )

    return {
        "response": response.choices[0].message.content,
        "is_streaming": False,
        "is_final": True,
    }

def _chat_with_streaming(self, messages, user_message, session_id):
    """
    Chat con streaming - risposta pezzo per pezzo
    """
    response_stream = openai.chat.completions.create(
        model="gpt-4.1-nano",
        messages=messages,
        max_tokens=150,
        stream=True,  # â† SÃŒ streaming
    )

    # Raccoglie tutti i pezzi
    complete_response = ""
    for chunk in response_stream:
        if chunk.choices[0].delta.content is not None:
            complete_response += chunk.choices[0].delta.content

    return {
        "response": complete_response,
        "is_streaming": True,
        "is_final": True,
    }
```

---

## ğŸš€ Implementazione Attuale vs Avanzata

### **Implementazione Attuale (Semplificata)**

-   âœ… **Funziona**: OpenAI streaming Ã¨ attivato
-   âœ… **Raccoglie**: Tutti i chunk vengono assemblati
-   âœ… **Salva**: Risposta completa nel database
-   â“ **Limitazione**: Ritorna solo la risposta finale

### **Implementazione Avanzata (Futura)**

Per un vero streaming real-time, dovresti:

1. **WebSockets** invece di HTTP normale
2. **Server-Sent Events (SSE)** per streaming HTTP
3. **Chunked Response** per inviare pezzi uno per volta

---

## ğŸ” Come Testare

### **Test 1: ModalitÃ  Normale**

1. Lascia il checkbox "Streaming" **NON** selezionato
2. Invia un messaggio
3. Vedi: "L'assistente sta pensando..."
4. Ricevi: Risposta completa

### **Test 2: ModalitÃ  Streaming**

1. Seleziona il checkbox "ğŸ“¡ Streaming"
2. Invia un messaggio
3. Vedi: "ğŸ“¡ Streaming in corso..."
4. Ricevi: Risposta con icona ğŸ”„

### **Debug Points:**

-   `chatbot_api/views.py:42` â†’ Controlla valore `streaming`
-   `chatbot_api/services.py:95` â†’ Vedi quale metodo viene chiamato
-   `chatbot_api/services.py:175` â†’ Debug del loop streaming

---

## ğŸ“Š Flusso Dati Streaming

```
1. UTENTE seleziona checkbox streaming âœ…
   â†“
2. FRONTEND invia streaming=true
   â†“
3. VIEWS estrae parametro streaming
   â†“
4. SERVICE sceglie _chat_with_streaming()
   â†“
5. OPENAI API chiamata con stream=True
   â†“
6. CHUNK loop assembla risposta completa
   â†“
7. DATABASE salva risposta completa
   â†“
8. FRONTEND riceve risposta con is_streaming=true
   â†“
9. UI mostra risposta con icona ğŸ”„
```

---

## ğŸ’¡ Miglioramenti Futuri

### **1. Real-time Streaming**

```python
# views.py - con Django Channels (WebSockets)
async def stream_chat(websocket):
    async for chunk in openai_stream:
        await websocket.send_text(chunk)
```

### **2. Server-Sent Events**

```python
# views.py - con SSE
def chat_stream(request):
    def event_stream():
        for chunk in openai_stream:
            yield f"data: {chunk}\n\n"

    return StreamingHttpResponse(event_stream(),
                                content_type='text/event-stream')
```

### **3. Progress Indicator**

```javascript
// Frontend con progress bar
let currentMessage = "";
eventSource.onmessage = function (event) {
    currentMessage += event.data;
    updateMessageInRealTime(currentMessage);
};
```

---

## ğŸ“ Concetti Appresi

### **Backend:**

-   âœ… **Parameter Validation**: Aggiungere nuovi campi ai serializers
-   âœ… **Method Routing**: Decisioni condizionali nei services
-   âœ… **OpenAI Streaming**: Usare `stream=True` e iterare chunks
-   âœ… **Error Handling**: Gestire errori in modalitÃ  diverse

### **Frontend:**

-   âœ… **Form Controls**: Checkbox e gestione stato
-   âœ… **Conditional UI**: Cambiare testo loading basato su modalitÃ 
-   âœ… **API Communication**: Inviare parametri aggiuntivi

### **Architecture:**

-   âœ… **Feature Flags**: Attivare/disattivare funzionalitÃ 
-   âœ… **Backwards Compatibility**: Nuove feature senza rompere esistenti
-   âœ… **Code Organization**: Separare logiche diverse in metodi separati

---

## ğŸ” Note di Sicurezza

âš ï¸ **Streaming e Rate Limiting:**

-   Lo streaming puÃ² consumare piÃ¹ risorse
-   Considera limitazioni per utente
-   Monitora uso token OpenAI

âš ï¸ **Timeout Handling:**

-   Stream possono essere piÃ¹ lunghi
-   Configura timeout appropriati
-   Gestisci disconnessioni client

---

**ğŸ¯ Obiettivo Didattico:**  
Capire come aggiungere nuove funzionalitÃ  a un'applicazione esistente mantenendo la compatibilitÃ  e seguendo le best practices! ğŸš€
